# 1.8 链接爬虫
　　以上我们已经实现了两种爬虫方式（网站地图和ID遍历爬虫），只要这两种技术可用，就应当使用其进行爬取，因为这两种方法最小化了需要下载的网页数量。不过，对于另一些网站，我们需要让爬虫表现得更像普通用户，跟踪链接，访问感兴趣的内容。
　　通过跟踪所有链接的方式，我们可以很容易地下载整个网站的页面，但其中大多数网页是我们不需要的。例如，我们要想从一个论坛中获取用户账号详情页，就只需下载账号页，而非讨论贴的页面。
　　下面我们将使用正则表达式来确定需要下载哪些页面：
```
# link_crawler
import urllib2
import re

def download(url, user_agent='wswp', num_retries=2):
	print 'Downloading:', url
	headers = {'User-agent': user_agent}
	request = urllib2.Request(url, headers=headers)
	try:
		html = urllib2.urlopen(request).read()
	except urllib2.URLError as e:
		print 'Download error:', e.reason
		html = None
		if num_retries > 0:
			if hasattr(e, 'code') and 500 <= e.code < 600:
				# retry 5XX HTTP errors
				return download(url, user_agent, num_retries - 1)
	return html

def link_crawler(seed_url, link_regex):
	"""Crawl from the given seed URL following links matched by link_regex
	"""
	crawl_queue = [seed_url]
	while crawl_queue:
		url = crawl_queue.pop()
		html = download(url)
		# filter for links matching our regular expression
		for link in get_links(html):
			if re.match(link_regex, link):
				crawl_queue.append(link)

def get_links(html):
	"""Return a list of links from html
	"""
	# a regular expression to extract all links from the webpage
	webpage_regex = re.compile('<a[^>]+href=["\'](.*?)["\']', re.IGNORECASE)
	# list of all links from the webpage
	return webpage_regex.findall(html)
```
***
### 相对链接与绝对链接
　　我们使用正则表达式/(index|view)/来匹配“国家列表索引页和国家页面”。
索引页连接格式如下：
* http://example.webscraping.com/index/1
* http://example.webscraping.com/index/2

国家页链接格式如下：
* http://example.webscraping.com/view/Afghanistan-1
* http://example.webscraping.com/view/Aland-Islands-2

运行结果：
```
asa@asa-virtual-machine:~$ python link_crawler.py 
Downloading: http://example.webscraping.com
Downloading: /index/1
Traceback (most recent call last):
...
ValueError: unknown url type: /index/1
```
　　出错，问题在于下载/index/1时，该链接只有网页的路径，没有协议和服务器部分，即它是一个<b>相对链接</b>。由于浏览器知道你正在浏览的网页，从此可以推断出相对链接，而urllib2不知道，也就无法定位相对链接对应的网页。
***
### 记录爬取链接，防止无限循环
　　因此，我们需要将相对链接转换为<b>绝对连接</b>。Python的urlparse模块可以实现该功能。下面我们使用urlparse来改进link_crawler：
```
import urlparse
def link_crawler(seed_url, link_regex):
	"""Crawler from the given seed URL following links matched by link_regex
	"""
	crawl_queue = [seed_url]
	while crawl_queue:
		url = crawl_queue.pop()
		html = download(url)
		for link in get_links(html):
			if re.match(link_regex, link):
				link = urlparse.urljoin(seed_url, link)
				crawl_queue.append(link)
```
运行结果：
```
asa@asa-virtual-machine:~$ python link_crawler.py 
Downloading: http://example.webscraping.com
Downloading: http://example.webscraping.com/index/1
Downloading: http://example.webscraping.com/index/2
Downloading: http://example.webscraping.com/index/3
...
Downloading: http://example.webscraping.com/index/24
Downloading: http://example.webscraping.com/index/25
Downloading: http://example.webscraping.com/index/24
Downloading: http://example.webscraping.com/index/25
...

```
　　没有错误，但是，当下载到某个页面时（24和25），开始进入循环下载的状态，这是因为他们之间存在彼此的链接，相互引用。比如，中国有韩国的链接，韩国又有中国的链接，爬虫就会在中国与韩国之间无限循环。
　　要想避免这种无限循环，就需要记录那些已经被爬取过的链接。对link_crawler函数改进如下：
```
import urlparse
def link_crawler(seed_url, link_regex):
	crawl_queue = [seed_url]
	# keep track which URL's have seen before
	seen = set(crawl_queue)
	while crawl_queue:
		url = crawl_queue.pop()
		html = download(url)
		for link in get_links(html):
			# check if link matches expected regex
			if re.match(link_regex,link):
				# form absolute link
				link = urlparse.urljoin(seed_url, link)
				# check if have already seen this link
				if link not in seen:
					seen.add(link)
					crawl_queue.append(link)
```
运行结果：
```
asa@asa-virtual-machine:~/桌面$ python link_crawler.py 
Downloading: http://example.webscraping.com
Downloading: http://example.webscraping.com/index/1
Downloading: http://example.webscraping.com/index/2
Downloading: http://example.webscraping.com/index/3
...
Downloading: http://example.webscraping.com/index/24
Downloading: http://example.webscraping.com/index/25
...
```
　　无限循环已解决，我们得到了一个可用的爬虫！
***
### 高级功能
#### 1. 解析robots.txt
通过http://example.webscraping.com/robots.txt ，我们获得robots.txt内容如下：
``` 
# section 1
User-agent: BadCrawler
Disallow: /

# section 2
User-agent: *
Crawl-delay: 5
Disallow: /trap 

# section 3
Sitemap: http://example.webscraping.com/sitemap.xml
```
　　首先，为了避免下载禁止爬取的URL，我们需要解析robots.txt。我们可以使用Python自带的robotparser模块，流程如下：
1. 使用robotparser模块加载robots.txt
2. 通过robotparser的can_fetch()函数，确定指定的<b>用户代理</b>是否允许访问网页

```
>>> import robotparser
>>> rp = robotparser.RobotFileParser()
>>> rp.set_url('http://example.webscraping.com/robots.txt')
>>> rp.read()
>>> url = 'http://example.webscraping.com'
>>> user_agent = 'BadCrawler'
>>> rp.can_fetch(user_agent, url)
False
>>> user_agent = 'GoodCrawler'
>>> rp.can_fetch(user_agent, url)
True
```
运行结果和robots.txt中定义的一样，当用户代理设置为'BadCrawler'时，无法获取网页。
为了将该功能集成到爬虫中，我们需要在crawl循环中添加该检查：
```
...
while crawl_queue:
		url = crawl_queue.pop()
		# check url passes robots.txt restrictions
		if rp.can_fetch(user_agent, url):
			...
		else:
			print 'Blocked by robots.txt:', url
```
#### 2. 支持代理

　　有时我们需要使用代理访问某个网站。比如，Netflix屏蔽了美国以外的大多数国家。使用urllib2支持代理并没有想象中那么容易（可以尝试使用更友好的Python HTTP模块requests来实现该功能，其文档地址为 http://docs.python-requests.org/） 。 以下是使用urllib2支持代理的代码：
```
proxy = ...
opener = urllib2.build_opener()
proxy_params = {urlparse.urlparse(url).scheme: proxy}
opener.add_handler(urllib2.ProxyHandler(proxy_params))
response = opener.open(request)
```

下面是集成了该功能的新版本download函数：
```
import urllib2
import robotparser

def download(url, user_agent='wswp', proxy=None, num_retries=2):
	print 'Downloading:', url
	headers = {'User-agent':user_agent}
	request = urllib2.Request(url,headers = headers)

	opener = urllib2.build_opener()
	if proxy:
		proxy_params = {urlparse.urlparse(url).scheme:proxy}
		opener.add_handler(urllib2.ProxyHandler(proxy_params))
	try:
		html = opener.open(request).read()
	except urllib2.URLError as e:
		print 'Download error:', e.reason
		html = None
		if num_retries > 0:
			if hasattr(e, 'code') and 500 <= e.code < 600:
			# retry 5XX HTTP errors
			html = download(url, user_agent, proxy, num_retries-1)
	return html
```

#### 3. 下载限速
　　防止下载速度过快导致封禁或服务器过载，我们要在两次下载之间添加延时，对下载进行限速：
```
class Throttle:
	"""Add a dalay between downloads to the same domain
	"""
	def __init__(self, delay):
		# amount of delay between downloads for each domain
		self.delay = delay
		# timestamp of when a domain was last accessed
		self.domains = {}

	def wait(self, url):
		domain = urlparse.urlparse(url).netloc
		last_accessed = self.domains.get(domain)

		if self.delay > 0 and last_accessed is not None:
			sleep_secs = self.delay - (datetime.datetime.now() - last_accessed).seconds
			if sleep_secs > 0:
				# domain has been accessed recently
				# so need to sleep
				time.sleep(sleep_secs)
			# update the last accessed time
			self.domains[domain] = datetime.datetime.now()
```
我们可以在每次下载之前，调用 throttle 对象对爬虫进行限速：
```
throttle = Throttle(delay)
...
throttle.wait(url)
result = download(url, headers, proxy = proxy, num_retries=num_retries)
```

#### 4. 避免爬虫陷阱
　　一些网站会动态生成页面内容，这样就会出现无限多的网页（如日历，就是一个用于无限链接的网页）。这就是<b>爬虫陷阱</b>。
　　我们需要限制链接数，也就是深度，来避免爬虫陷阱，通过修改 seen 变量来实现这一功能：
```
def link_crawler(..., max_depth=2):
	max_depth = 2
	seen = {}
	...
	depth = seen[url]
	if depth != max_depth:
		for link in links:
			if link not in seen:
				seen[link] = depth + 1
				crawl_queue.append(link)
```
　　此时，我们的爬虫就很好的避免了爬虫陷阱。将 max_depth 设为一个负数，可以禁用此功能。

#### 5. 最终版本
未完待续……