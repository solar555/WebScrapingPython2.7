# 1.5 编写第一个网络爬虫
　　为了抓取网站，我们首先需要下载包含有感兴趣数据的网页，该过程一般被称为爬取（Crawling）。爬取网站的方法有很多，而使用哪种方法取决于目标网站的结构。
　　我们首先探讨如何安全地下载网页，然后介绍如下3种爬取网站的常见方法：
* 爬取网站地图；
* 遍历每个网页的数据库ID；
* 跟踪网页链接。
## 一、下载网页
### （一）下载

要爬取，先下载。以下脚本使用Python的urllib2模块下载URL：
```
#!/bin/bash
# author:Asa

import urllib2
def download(url):
        print 'Downloading:',url
        try:
                html = urllib2.urlopen(url).read()
        except urllib2.URLError as e:
                print 'Download error:', e.reason
                html = None
        return html
```
### （二）重新下载
常见错误：
* 4xx（如404）：请求存在问题时，发生4xx错误
* 5xx（如503）：服务端存在问题时，发生5xx错误
我们只需要确保download函数在发生5xx错误时重试下载即可：

```
#!/bin/bash
# author:Asa
# url:www.redcross.com

import urllib2
def download(url, num_retries=2):
        print 'Downloading:', url
        try:
                html=urllib2.urlopen(url).read()
        except urllib2.URLError as e:
                print 'Download error:', e.reason
                html=None
                if num_retries > 0:
                        if hasattr(e, 'code') and 500 <= e.code < 600:
                                # recursively retry 5xx HTTP errors
                                return download(url, num_retries-1)
                return html
```

调用结果如下：
```
>>> download('http://httpstat.us/500')
Downloading: http://httpstat.us/500
Download error: Internal Server Error
Downloading: http://httpstat.us/500
Download error: Internal Server Error
Downloading: http://httpstat.us/500
Download error: Internal Server Error

```
步骤：
1. 先尝试下载。
2. 收到500错误后，又进行了两次重试才放弃。

### （三）设置用户代理
　　默认情况下，urllib2使用Python-urllib/2.7作为用户代理下载网页内容，其中2.7是Python的版本号。如果能使用可辨识的用户代理则更好，这样可以避免网络爬虫碰到一些问题。
　　此外，也许是因为曾经经历过质量不佳的Python网络爬虫造成的服务器过载，一些网站还会封禁这个默认的用户代理。
　　因此，为了下载更加可靠，我们需要控制用户代理的设定。下面我们修改download代码，设定一个默认的用户代理“wswp”（Web Scraping with Python）：
```
#!/bin/bash
# author:Asa

import urllib2
def download(url, user_agent='wswp', num_retries=2):
	print 'Downloading:', url
	headers = {'User-agent': user_agent}
	request = urllib2.Request(url, headers=headers)
	try:
		html = urllib2.urlopen(request).read()
	except urllib2.URLError as e:
		print 'Download error:', e.reason
		html = None
		if num_retries > 0:
			if hasattr(e, 'code') and 500 <= e.code < 600:
				# retry 5XX HTTP errors
				return download(url, user_agent, num_retries - 1)
	return html
```