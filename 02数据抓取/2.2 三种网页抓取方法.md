# 2.2 三种网页抓取方法
## 2.2.1 正则表达式
```
import re
import urllib2

def download(url, user_agent='wswp', num_retries=2):
	print 'Downloading:', url
	headers = {'User-agent': user_agent}
	request = urllib2.Request(url, headers=headers)
	try:
		html = urllib2.urlopen(request).read()
	except urllib2.URLError as e:
		print 'Download error:', e.reason
		html = None
		if num_retries > 0:
			if hasattr(e, 'code') and 500 <= e.code < 600:
				# retry 5XX HTTP errors
				return download(url, user_agent, num_retries - 1)
	return html

url = 'http://example.webscraping.com/view/United-Kingdom-239'
html = download(url)
result = re.findall('<td class="w2p_fw">(.*?)</td>',html)
print result
```

## 2.2.2 Beautiful Soup
```
from bs4 import BeautifulSoup
import urllib2

def download(url, user_agent='wswp', num_retries=2):
	print 'Downloading:', url
	headers = {'User-agent': user_agent}
	request = urllib2.Request(url, headers=headers)
	try:
		html = urllib2.urlopen(request).read()
	except urllib2.URLError as e:
		print 'Download error:', e.reason
		html = None
		if num_retries > 0:
			if hasattr(e, 'code') and 500 <= e.code < 600:
				# retry 5XX HTTP errors
				return download(url, user_agent, num_retries - 1)
	return html

url = 'http://example.webscraping.com/places/default/view/United-Kingdom-239'
html = download(url)

soup = BeautifulSoup(html)
tr = soup.find(attrs={'id':'places_area__row'})
td = tr.find(attrs={'class':'w2p_fw'})
area = td.text

print area
```

## 2.2.3 Lxml
1.解析一个不完整HTML的例子
```
import lxml.html
broken_html = '<ul class=country><li>Area<li>Population</ul>'
tree = lxml.html.fromstring(broken_html) # parse the HTML
fixed_html = lxml.html.tostring(tree, pretty_print=True)
print fixed_html
```

2.使用lxml的CSS选择器
```
import lxml.html
import urllib2

def download(url, user_agent='wswp', num_retries=2):
	print 'Downloading:', url
	headers = {'User-agent': user_agent}
	request = urllib2.Request(url, headers=headers)
	try:
		html = urllib2.urlopen(request).read()
	except urllib2.URLError as e:
		print 'Download error:', e.reason
		html = None
		if num_retries > 0:
			if hasattr(e, 'code') and 500 <= e.code < 600:
				# retry 5XX HTTP errors
				return download(url, user_agent, num_retries - 1)
	return html

url = 'http://example.webscraping.com/places/default/view/United-Kingdom-239'
html = download(url)

tree = lxml.html.fromstring(html)
td = tree.cssselect('tr#places_area__row > td.w2p_fw')[0]

area = td.text_content()
print area
```

## 2.2.4 性能对比
### 源码：
```
FIELDS = ('area', 'population', 'iso', 'country', 'capital', 
	'continent', 'tld', 'currency_code', 'currency_name', 'phone'
	'postal_code_format', 'postal_code_regex', 'languages',
	'neighbours')

import urllib2
def download(url, user_agent='wswp', num_retries=2):
	print 'Downloading:', url
	headers = {'User-agent': user_agent}
	request = urllib2.Request(url, headers=headers)
	try:
		html = urllib2.urlopen(request).read()
	except urllib2.URLError as e:
		print 'Download error:', e.reason
		html = None
		if num_retries > 0:
			if hasattr(e, 'code') and 500 <= e.code < 600:
				# retry 5XX HTTP errors
				return download(url, user_agent, num_retries - 1)
	return html
 
# 1.使用正则表达式
import re
def re_scraper(html):
	results = {}
	for field in FIELDS:
		result = re.search('<tr id="places_%s__row">.*?<td class="w2p_fw">(.*?)</td>' % field, html)
		if result == None:
			continue

	 	groups = result.groups()
		if (groups == None) | (len(groups) == 0):
			continue

		results[field] = groups[0]
	return results

# 2.使用BeautifulSoup
from bs4 import BeautifulSoup
def bs_scraper(html):
	soup = BeautifulSoup(html, 'html.parser')
	results = {}
	for field in FIELDS:
		result = soup.find('table')
		if result == None:
			continue

		result_find = result.find('tr',id='places_%s__row' % field)
		if result_find == None:
			continue

		result_find_find = result_find.find('td', class_='w2p_fw')
		if result_find_find == None:
			continue

		results[field] = result_find_find.text
	return results

# 3.使用Lxml
import lxml.html
def lxml_scraper(html):
	tree = lxml.html.fromstring(html)
	results = {}
	for field in FIELDS:
		tds = tree.cssselect('table > tr#places_%s__row > td.w2p_fw' % field)
		if len(tds) == 0:
			continue

		td = tds[0]
		if td == None:
			continue

		results[field] = td.text_content()
	return results

# 开始测试
import time
NUM_ITERATIONS = 500 # number of times to test each scraper
html = download('http://example.webscraping.com/places/default/view/United-Kingdom-239')

for name, scraper in [('Regular expressions', re_scraper),
	('BeautifulSoup', bs_scraper),
	('Lxml', lxml_scraper)]:
	# record start time of scrape
	start = time.time()

	for i in range(NUM_ITERATIONS):
		if scraper == re_scraper:
			re.purge()
		result = scraper(html)
		# check scraped result is as expected
		assert(result['area'] == '244,820 square kilometres')
	# record end time of scrape and output the total
	end = time.time()
	print '%s: %.2f seconds' % (name, end - start)
```

### 结果：
```
Downloading: http://example.webscraping.com/places/default/view/United-Kingdom-239
Regular expressions: 1.34 seconds
BeautifulSoup: 19.64 seconds
Lxml: 3.65 seconds

```

## 2.2.5 结论
|抓取方法|性能|使用难度|安装难度|
|-|-|-|-|
|正则表达式|快|困难|简单（内置模块）|
|Beautiful Soup|慢|简单|简单（纯Python）|
|Lxml|快|简单|相对困难|

### 建议：
1. 如果你的爬虫瓶颈是下载网页，而不是抽取数据，使用较慢的方法（如Beautiful Soup）也不成问题。
2. 如果只需抓取少量数据，且想要避免额外依赖的话，正则表达式可能更加合适。
3. 但通常情况下，lxml是抓取数据的最好选择，它即快速又健壮，而正则和Beautiful Soup只在某些特定场景下有用。

## 2.2.6 为链接爬虫添加抓取回调
### 抓取数据并显示
```
FIELDS = ('area', 'population', 'iso', 'country', 'capital', 
	'continent', 'tld', 'currency_code', 'currency_name', 'phone'
	'postal_code_format', 'postal_code_regex', 'languages',
	'neighbours')

import urllib2
def download(url, user_agent='wswp', num_retries=2):
	print 'Downloading:', url
	headers = {'User-agent': user_agent}
	request = urllib2.Request(url, headers=headers)
	try:
		html = urllib2.urlopen(request).read()
	except urllib2.URLError as e:
		print 'Download error:', e.reason
		html = None
		if num_retries > 0:
			if hasattr(e, 'code') and 500 <= e.code < 600:
				# retry 5XX HTTP errors
				return download(url, user_agent, num_retries - 1)
	return html

import re
import lxml.html
def scrape_callback(url, hmtl):
	if re.search('/view/', url):
		tree = lxml.html.fromstring(html)
		row = []
		for field in FIELDS:
			tds = tree.cssselect('table > tr#palaces_%s__row > td.w2p_fw' % field)
			if len(tds) == 0:
				continue

			td = tds[0]
			if td == None:
				continue

			content = td.text_content()
			row.append(content)

		print url, row

url = 'http://example.webscraping.com/places/default/view/United-Kingdom-239'
html = download(url)
scrape_callback(url, html)
```

### 抓取数据并保存到CSV表格中
```
import csv 
import urllib2
import re
import urlparse
import robotparser
import lxml.html

class ScrapeCallback:
	def __init__(self):
		self.writer = csv.writer(open('countries.csv', 'w'))
		self.fields = ('area', 'population', 'iso', 'country', 'capital', 
			'continent', 'tld', 'currency_code', 'currency_name', 'phone'
			'postal_code_format', 'postal_code_regex', 'languages', 'neighbours')
		self.writer.writerow(self.fields)

	def __call__(self, url, html):
		if re.search('/view/', url):
			tree = lxml.html.fromstring(html)
			row = []
			for field in self.fields:
				tds = tree.cssselect('table > tr#places_{}__row > td.w2p_fw'.format(field))
				if len(tds) == 0:
					continue

				row.append(tds[0].text_content())
			self.writer.writerow(row)

def download(url, user_agent='wswp', num_retries=2):
	print 'Downloading:', url
	headers = {'User-agent': user_agent}
	request = urllib2.Request(url, headers=headers)
	try:
		html = urllib2.urlopen(request).read()
	except urllib2.URLError as e:
		print 'Download error:', e.reason
		html = None
		if num_retries > 0:
			if hasattr(e, 'code') and 500 <= e.code < 600:
				# retry 5XX HTTP errors
				return download(url, user_agent, num_retries - 1)
	return html

def link_crawler(seed_url, link_regex, scrape_callback, user_agent='wswp'):
	rp = robotparser.RobotFileParser()
	crawl_queue = [seed_url]

	# keep track which URL's have seen before
	seen = set(crawl_queue)
	while crawl_queue:
		url = crawl_queue.pop()

		# check url passes robots.txt restrictions
		#if rp.can_fetch(user_agent, url):
		html = download(url)
		for link in get_links(html):
			# check if link matches expected regex
			if re.search(link_regex,link):
				# form absolute link
				link = urlparse.urljoin(seed_url, link)
				# check if have already seen this link
				if link not in seen:
					crawl_queue.append(link)
					html_link = download(link)
					scrape_callback(link, html_link)
		#else:
			#print 'Blocked by robots.txt:', url

def get_links(html):
	"""Return a list of links from html
	"""
	# a regular expression to extract all links from the webpage
	webpage_regex = re.compile('<a[^>]+href=["\'](.*?)["\']', re.IGNORECASE)
	# list of all links from the webpage
	return webpage_regex.findall(html)
	
link_crawler('http://example.webscraping.com/', '/(view)', scrape_callback=ScrapeCallback())
```
### 打开CVS文件，可以看到抓取的数据。